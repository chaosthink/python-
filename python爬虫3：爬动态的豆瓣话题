# -*- coding: utf-8 -*-
"""
Created on Wed Sep 25 11:13:43 2019

@author: fangzhaoxing
"""

"""
分析了下：

豆瓣话题用的是动态页面，加载不出页码
抓包的方式的话也会被400拒绝
考虑使用模拟滚动+正则的方式爬取
"""

from selenium import webdriver
import time
import json
import re
import numpy as np
import pandas as pd

"""
做模拟滚动前要先下载 selenium包
还需要下载浏览器对应的driver，我用的是chrome，搜chromedrive镜像，找到对应版本下载即可
然后把drive放到chrome安装目录
"""
##读取driver路径
browser = webdriver.Chrome(executable_path="D:\\Chrome-bin\\chromedriver.exe")
###get url
browser.get("https://www.douban.com/gallery/topic/104590/?from_reason=%E5%88%9A%E5%88%9A%E5%8F%82%E4%B8%8E&from=gallery_rec_topic")
###设置下休眠时间，缓冲话题
time.sleep(5)

#爬10页，每次停留20秒好了
###豆瓣有点贱，直接模拟到底部不给爬，只好上下滚动
for i in range(10):
    browser.execute_script("window.scrollTo(0, document.body.scrollHeight); var lenOfPage=document.body.scrollHeight; return lenOfPage;")
    time.sleep(5)
    browser.execute_script("window.scrollTo(0, document.body.scrollHeight); var q=document.documentElement.scrollTop=0;")
    time.sleep(15)

####\n数据抓不出来，替换掉，空白数据仅有图片的也先替换成，号
test = browser.page_source.replace('\n','')
test2 = test.replace('<pre class="status-full hide"></pre>','<pre class="status-full hide">,,</pre>')

####爬取用户空间和用户姓名
reg_username = '<a class="author" href=(.+?)</a><span class="type">'   
user_name = re.compile(reg_username)
user_namelist = re.findall(user_name,test2)

username = []
for i in range(len(user_namelist)):
    username.append(user_namelist[i].split('target="_blank">'))

####爬取每条记录的详细链接和发表时间
reg_usertime = '<time class="time"><a href=(.+?)</a></time>'   
user_time = re.compile(reg_usertime)
user_timelist = re.findall(user_time,test2)

usertime = []
for i in range(len(user_timelist)):
    usertime.append(user_timelist[i].split('target="_blank">'))

####爬取类型
reg_type = '<span class="type">的(.+?)</span><time'   
ht = re.compile(reg_type)
typelist = re.findall(ht,test2)

###爬取正文
reg_body = '<pre class="status-full hide">(.+?)</pre>'   
body = re.compile(reg_body)
bodylist = re.findall(body,test2)

####合并爬取的数据
username_df = pd.DataFrame(username,columns = ['space','username'])
usertime_df =  pd.DataFrame(usertime,columns = ['detail_url','happentime'])
typelist_df =  pd.DataFrame(typelist,columns = ['typelist'])
df_concat = pd.concat( [username_df, usertime_df, typelist_df], axis=1 )

####因为爬取话题内的日记，还需要进链接，所以下次再研究，大部分是广播，先爬广播
df_concat = df_concat[df_concat.typelist == '广播']
df_concat = df_concat.reset_index(drop=True)
bodylist =  pd.DataFrame(bodylist,columns = ['body'])
df_concat_final = pd.concat( [df_concat, bodylist], axis=1 )

df_concat_final.to_csv('C:\\Users\\test2\\Desktop\\fangzi.csv',encoding='utf_8_sig')

####关闭浏览器
browser.quit() 
