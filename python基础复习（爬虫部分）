# -*- coding: utf-8 -*-
"""
Created on Sat Sep  7 16:09:42 2019

@author: fangzhaoxing
"""

####
"""
   python 爬虫部分

"""
###


#####爬虫就是模拟人的网页浏览行为
                    
#####网页源代码的结构

"""
html是网页的主体
css是负责对网页进行装饰的
JavaScript是网页的功能
爬虫只需要了解html
html是标签型的语言，以标签对进行出现
<表示开始> </表示结束>
<head> 是网页的前后部分，不包含主体
 <body> 包含网页的主体内容
  <body>里包含 div区块，区块里又包含不同的属性
   <h1> 不同字号标题
    <li> 列表
     <img> 图片
      <a>放置链接
       <p> 文字内容
"""

#写一个html文件
"""
<!DOCTYPE html>
<html>
    <head>
        <title> test title </title>
            <body>
                <div>
                    <h1>逗总是个大傻瓜</h1>
                    <h2>数据服务</h2>
                    <p>努力成为一个厉害的数据分析师</p> 
                </div>
                <div>
                    <li><a href="https://fanyi.baidu.com/">百度翻译</a>
                    </li>
                    <li><a href="https://www.runoob.com/">小木虫</a>
                    </li>
                </div>
            </body>
    </head>
</html>
"""

###htnl结构  根节点跟叶节点的结构，再理解为父节点跟子节点
#####理解请求和响应的关系
"""
Request 请求 向服务器发送访问请求
    请求包含 get方式 ：大多数网站用的方式，响应速度快
            post方式 除了查询信息外还可以修改信息---提交信息的时候，如提交密码 ，翻译内容等（需要构建表单）
              写爬虫前要确认向谁发送请求，用什么方式请求
              user-agent  显示浏览的方式，称为请求头 ，爬虫时会显示python查询
              
Response 响应 服务器在接受用户的请求后，会验证请求的有效性，然后向用户发送响应的内容
        响应情况：200：成功返回  404：请求网页不存在  403：服务器拒绝请求

"""

### 写第一个爬虫
######urllib
##tieba.baidu.com/p/2460150866   桌面把
"""
打开贴吧，点击帖子，点击一个图片检查，可以看到图片的地址
双击图片，复制链接看是不是对应的图片
在网页源代码找到图片地址，观察前后的格式
"""

import urllib.request

####urlopen 
url = "https://tieba.baidu.com/p/6105059826 "
response = urllib.request.urlopen(url)
####response是一个htttp
html = response.read().decode('utf-8')

import re ####
###re.findall  re.search  re.copile
####re.findall  找到所有符合表达式的字符串，有序返回列表
test_test = '今年是2019年，是我学习数据分析的第1年'
re.findall('\d+' , test_test)  ####\d在正则表达式中表示数个数字串
re.findall('(\d+.)' , test_test)  ####\d在正则表达式中表示数个数字串

"""
基础的正则
. 除换行符 \n 外任意字符
\ 转义字符
\d 数字
{m} 匹配前一个字符m次
.+？ 匹配任意字符一次货无限次
（...） 括号来表示分组，多次使用 编号+1
"""
test_test = '今年我的2019年，是我学习数据分析的第1年'
re.findall('是我(.+?)数据',test_test)
re.findall('我(..)数据',test_test)
re.findall('今(.+?)是',test_test)  #####提取前后数据

###re.compile   根据正则表达式创建一个模式对象，可以更高效的匹配字符串
reobj = re.compile('(\d+.)')  ####使用模式对象，效率更高
reobj.findall(test_test)

reg = 'src="(.+?\.jpg)" size'
imgrwe = re.compile(reg)
imglist = re.findall(imgrwe,html)
####把url保存到本地
import time
x = 0
for imgurl in imglist:
    #print(imgurl)
    urllib.request.urlretrieve(imgurl,'C:\\Users\\fangzhaoxing\\Desktop\\test\\pic_%s.jpg' %x)
    x+=1
    time.sleep(2)  ####延长时间，避免被封
    print(x)
print('done')

#￥####
#########第三方库爬虫
###requests
import requests
url = "https://tieba.baidu.com/p/6105059826 "
response = requests.get(url)  ##### 包含了打开url  读取超文本  编码超文本 
response.text 
reg = 'src="(.+?\.jpg)" size'
imgrwe = re.compile(reg)
imglist = re.findall(imgrwe,response.text)

import time
x = 0
for imgurl in imglist:
    #print(imgurl)
    urllib.request.urlretrieve(imgurl,'C:\\Users\\fangzhaoxing\\Desktop\\test\\pic_%s.jpg' %x)
    x+=1
    time.sleep(2)  ####延长时间，避免被封
    print(x)
print('done')


#####使用BeautifulSoup 进行关键字的提取
from bs4 import BeautifulSoup
#####BeautifulSoup 提取网页的标签
####先构造一个html格式
html_doc = """
<!DOCTYPE html>
<html>
    <head>
        <title> test title </title>
            <body>
                <div>
                    <h1>逗总是个大傻瓜</h1>
                    <h2>数据服务</h2>
                    <p>努力成为一个厉害的数据分析师</p> 
                </div>
                <div>
                    <li><a href="https://fanyi.baidu.com/">百度翻译</a>
                    </li>
                    <li><a href="https://www.runoob.com/">小木虫</a>
                    </li>
                </div>
            </body>
    </head>
</html>
"""
soup = BeautifulSoup(html_doc)  ####使用默认的解析器
soup = BeautifulSoup(html_doc,'lxml')  ####lxml解析器解析能力强
soup  ###跟刚才差不多，只是结构变化了
soup.title  #$###读取标题
soup.title.name
soup.title.string
soup.title.parent.name
soup.h1
soup.p
soup.a
soup.find_all('a')
soup.find(a='百度翻译')  #$####精准查找
for link in soup.find_all('a'):
    print(link.get('href')) ####获取链接
soup.get_text()
soup.get()


######试着爬中国旅游网
url = 'http://www.cntour.cn/'
response = requests.get(url) 
response.text
reg = 'class="top"><a target="_blank" href="(.+?)>'
imgrwe = re.compile(reg)
imglist = re.findall(imgrwe,response.text)

reg2 = '(.+?)/"'
imgrwe2 = re.compile(reg2)

reg3 = ' title="(.+?)"'
imgrwe3 = re.compile(reg3)

html_total = []
word_total = []
for i in list(imglist):
    html_total.append(str(re.findall(imgrwe2,i)))
    word_total.append(str(re.findall(imgrwe3,i)))
html_total = pd.Series(html_total)
word_total = pd.Series(word_total)
d = {'html':html_total , 'word' : word_total}
data_total = pd.DataFrame(d)
type(html_total[0])


data_total.to_csv('C:\\Users\\fangzhaoxing\\Desktop\\test\\test.csv',encoding='utf_8_sig')

######上面是自己的代码
####下面是老师的实现
from bs4 import BeautifulSoup
url = 'http://www.cntour.cn/'
strhtml = requests.get(url)
strhtml.text
soup  = BeautifulSoup(strhtml.text,'lxml')  ###使用beautifulsoup 将网页的格式转化
data = soup.select('#main > div > div.mtop.firstMod.clearfix > div.centerBox > ul.newsList > li.top > a')
####清洗和组织数据
data[0].get('href')
data[0].get('title')
for item in data:
    result = {
            'news':item.get_text(),
            'html':item.get('href')
            }
result

######内容缺失了，重新温习一下
import requests
import json
def  get_translate(word=None):    ####定义一个翻译函数,词语默认为空
    url = 'http://fanyi.youdao.com/translate?smartresult=dict&smartresult=rule'   ####有道翻译的链接
    payload = {     ####表单数据
        'i': word,
        'from': 'AUTO',
        'to': 'AUTO',
        'smartresult': 'dict',
        'client': 'fanyideskweb',
        'salt': '15681303074613',
        'sign': 'ae245cb9109fb967e7dc41c37d5e84a6',
        'ts': '1568130307461',
        'bv': '53539dde41bde18f4a71bb075fcf2e66',
        'doctype': 'json',
        'version': '2.1',
        'keyfrom': 'fanyi.web',
        'action': 'FY_BY_REALTlME'
         }
    response = requests.post(url,data=payload)   ####使用post的方式，并且把表单传进来
    content = json.loads(response.text) 
    print(content['translateResult'][0][0]['tgt'] )   ####用json的方式读到翻译结果那一层
get_translate('测试')
if __name__=='__main__':
    get_translate('小花钱包')
if __name__=='__main__':  #####表示函数只会执行下面的代码，不会执行全部的代码
    get_translate('你知不知道')
    
######尝试进行反爬
####找到user_agent ，将请求头放到一个列表里，进行伪装
headers = {'User-Agent':'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/75.0.3770.142 Safari/537.36'}
import time
####把标签头放进一个字典里，伪装自己的来源
    payload = {
        'i': word,
        'from': 'AUTO',
        'to': 'AUTO',
        'smartresult': 'dict',
        'client': 'fanyideskweb',
        'salt': '15681303074613',
        'sign': 'ae245cb9109fb967e7dc41c37d5e84a6',
        'ts': '1568130307461',
        'bv': '53539dde41bde18f4a71bb075fcf2e66',
        'doctype': 'json',
        'version': '2.1',
        'keyfrom': 'fanyi.web',
        'action': 'FY_BY_REALTlME'
         }
    response = requests.post(url,data=payload,headers = headers)  ####新加入heraders进行伪装
    time.sleep(2) ####加进休息时间
    content = json.loads(response.text)
    print(content['translateResult'][0][0]['tgt'] )
    
######使用代理ip池进行伪装
####代理ip地址：https://www.xicidaili.com/  免费的不会太好用
proxies = {
      'https': 'http://163.204.240.171:9999' }

    payload = {
        'i': word,
        'from': 'AUTO',
        'to': 'AUTO',
        'smartresult': 'dict',
        'client': 'fanyideskweb',
        'salt': '15681303074613',
        'sign': 'ae245cb9109fb967e7dc41c37d5e84a6',
        'ts': '1568130307461',
        'bv': '53539dde41bde18f4a71bb075fcf2e66',
        'doctype': 'json',
        'version': '2.1',
        'keyfrom': 'fanyi.web',
        'action': 'FY_BY_REALTlME'
         }
    response = requests.post(url,data=payload,headers = headers,proxies=proxies)  ####将代理ip放到这里进行伪装
    time.sleep(2) ####加进休息时间
    content = json.loads(response.text)
    print(content['translateResult'][0][0]['tgt'] )

######以上是简单的反扒技巧

####下面开始爬淘宝的下拉列表
url = 'https://suggest.taobao.com/sug?q=%E6%B0%B4%E6%9D%AF&code=utf-8&area=c2c&nick=&sid=null'    ####找到淘宝一级链接的地址
strheml = requests.get(url)  ####读取url
strheml.text   ####已经读到txt来了
str_json= json.loads(strheml.text)  ####读取文档
str_json
str_json['result'][0][0]  ###已经可以提取一级关键字
str_json['result'][0]

###使用占位符替代关键字
keyword = '电脑'
url = 'https://suggest.taobao.com/sug?q={}&code=utf-8&area=c2c&nick=&sid=null'.format(keyword) ####使用调用外部字符
strheml = requests.get(url)
str_json= json.loads(strheml.text)
str_json['result']

###循环爬取二级下拉词
keyword = '电脑'
url = 'https://suggest.taobao.com/sug?q={}&code=utf-8&area=c2c&nick=&sid=null'.format(keyword) ####使用调用外部字符
strheml = requests.get(url)
str_json= json.loads(strheml.text)
str_json['result']
for item in str_json['result']:
    print(item[0])  ####先打印一波二级下拉词
total_list = []
for item in str_json['result']:  ###利用循环传入参数，批量读取
    time.sleep(2)
    url = 'https://suggest.taobao.com/sug?q={}&code=utf-8&area=c2c&nick=&sid=null'.format(item[0])
    gethtml = requests.get(url)
    str_json = json.loads(gethtml.text)
    for v in str_json['result']:
        total_list.append(v)
    print(total_list)
    

####把爬到的数据保存到数据集
import requests
import json
import pandas as pd
result = {'标题':'标题','权重':'权重','层级':'层级'}
keyword = '电脑'
url = 'https://suggest.taobao.com/sug?q={}&code=utf-8&area=c2c&nick=&sid=null'.format(keyword)
strhtml = requests.get(url)
str_json = json.loads(strhtml.text)
for item in str_json['result']:
    result['标题']=item[0]
    result['权重']=item[1]
    result['层级'] = 1
    data = pd.DataFrame.from_dict(result,orient='index').T
    data.to_csv('C:\\Users\\fangzhaoxing\\Desktop\\test.csv',index=False,header=False,mode='a+',encoding='gbk')


#####调用api接口
###保存key值  46af150f27e54c51846e4d73e5f9f959
    
###开始学习调用api
key = '46af150f27e54c51846e4d73e5f9f959'
city = 'beijing'  ####测试api的调用
url = 'https://api.heweather.net/s6/weather/forecast?location={}&key={}'.format(city,key)
strhtml=requests.get(url)
strhtml.text
str_json = json.loads(strhtml.text)

str_json['HeWeather6'][0]['basic']['location'] ####获取城市
str_json['HeWeather6'][0]['daily_forecast'][0]
str_json['HeWeather6'][0]['daily_forecast'][0]['tmp_max']
#######把天气写到一行
result = {'city':'','date':'','cond':'','max':'','min':''}

for item in str_json['HeWeather6'][0]['daily_forecast']:
    result['city']=str_json['HeWeather6'][0]['basic']['location']
    result['date']=item['date']
    result['cond']=item['cond_txt_n']
    result['max'] = item['tmp_max']
    result['min'] = item['tmp_min']
    
####自动爬取各大城市天气
city_list = pd.read_csv('D:\\Users\\fangzhaoxing\\Downloads\\china-city-list.csv', skiprows=1)  ###读取数据跳过一行

city_list.City_EN.iloc[1]

final_list = pd.DataFrame()
for  i  in range(20):
    key = '46af150f27e54c51846e4d73e5f9f959'
    city = city_list.City_EN.iloc[i]
    url = 'https://api.heweather.net/s6/weather/forecast?location={}&key={}'.format(city,key)
    strhtml=requests.get(url)
    str_json = json.loads(strhtml.text)
    result = {'city':'','date':'','cond':'','max':'','min':''}
    for item in str_json['HeWeather6'][0]['daily_forecast']:
        result['city']=str_json['HeWeather6'][0]['basic']['location']
        result['date']=item['date']
        result['cond']=item['cond_txt_n']
        result['max'] = item['tmp_max']
        result['min'] = item['tmp_min']
    result_pd = pd.DataFrame(result,index=[i])
    final_list = pd.concat([result_pd,final_list])
    print(i)

